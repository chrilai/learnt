{
 "metadata": {
  "name": "clai15_PR9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Module 9 - Programming Assignment\n=\nThis is the notebook for the Module 9 Programming Assignment.\n\nHere are a few tips for using the iPython HTML notebook:\n\n1.  You can use tab . Try le<&lt;tab> and see the available functions.\n2.  You can change the type of cell by picking \"Code\" or \"Markdown\" from the menu at the left.\n3.  If you keep typing in a Markdown text area, you will eventually get scroll bars. To prevent this, hit return when you come to\nthe end of the window. Only a double return creates a new paragraph.\n4.  You can find out more about Markdown text here: http://daringfireball.net/projects/markdown/ (Copy this link and put it \nin another tab for reference--Don't click it or you'll leave your notebook!).\n5.  Every so often, restart the kernel, clear all output and run all code cells so you can be certain that you didn't\ndefine something out of order.\n\n**You should rename this notebook to be &lt;your JHED id>_PR9\\.ipynb** Do it right now.\n\n**Make certain the entire notebook executes before you submit it.** (See Hint #5 above)\n\nChange the following variables:"
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": "name = \"Christine Lai\"\njhed_id = \"clai15\"\nif name == \"Student Name\" or jhed_id == \"sname1\":\n    raise Exception( \"Change the name and/or JHED ID...preferrably to yours.\")",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 129
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Add whatever additional imports you require here. Stick with the standard libraries and those required by the class. The import\ngives you access to these functions: http://ipython.org/ipython-doc/stable/api/generated/IPython.core.display.html (Copy this link)\nWhich, among other things, will permit you to display HTML as the result of evaluated code (see HTML() or display_html())."
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": "from IPython.core.display import *\nfrom StringIO import StringIO\nfrom math import ceil # math module ceiling function\nfrom random import shuffle # random module list shuffling function\nimport matplotlib.pyplot as plt # pyplot module",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 130
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "In the last programming assignment, you implemented both linear and logistic regression. In this assignment, you're going to use the performance metrics and model evaluation techniques discussed in this module to evaluate the logistic regression.\n\nYou should use the logistic_regression.csv data from last time. You will also need to use your logistic regression implementation. The cleanest way is to export that notebook to a regular .py file and import your logistic_regression() function. You will need to submit that file along with your notebook. Otherwise, you'll have to copy-paste just the code you need into this workbook."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from clai15_PR8 import read_csv, logistic_y_hat, logistic_regression",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 131
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "In order to evaluate the logistic regression on the logistic data, you're going to need to implement 5-fold cross validation and averaged performance metrics (accuracy, error, precision, recall). You will also need to implement learning curves. These are all discussed in the lecture. If you have questions, post them in the discussion forum...more questions than usual are to be expected since this topic is not covered in the book.\n\nThe questions you want to answer are:\n\n1. how well would the logistic regression perform on new data\n2. will getting more training data improve performance\n3. What is the right parameter setting for the logistic regression.\n    \n5-fold cross validation and learning curves all answer these questions. You will use the rest of this notebook as \"lab notes\", alternating between implementing code with comments and a running discussion of the evaluation steps and answering the questions.\n\nUse matplotlib for your charts; a student posted a link to a nice tutorial http://matplotlib.org/1.3.1/users/pyplot_tutorial.html."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Split a set of data into $n$ equal lists and return the data as a list of length $n$. Each of the sublists will represent a test/training fold."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_split_data(data, n):\n    split_data = [[] for i in range(n)]\n    for index in range(len(data)):\n        split_data[index%n].append(data[index])\n    return split_data\n\ntest_data = [1,2,3,4,5,6,7,8,9,10]\nprint get_split_data(test_data, 3) #test",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "[[1, 4, 7, 10], [2, 5, 8], [3, 6, 9]]\n"
      }
     ],
     "prompt_number": 132
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Find and return estimated $\\hat{y}=\\frac{1}{1+e^{-\\theta\\cdot x}}\\rightarrow\\{0|1\\}$ values with the $\\theta$ vector found by logistic regression. These will be compared with the actual $y$ values."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_y_hat(theta, data, threshold=0.5):\n    y_hat = []\n    for value in logistic_y_hat(theta, data):\n        if value < threshold:\n            y_hat.append(0)\n        else:\n            y_hat.append(1)\n    return y_hat",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Compare the predicted values $\\hat{y}$ against the actual values $y$. Count the number of true and false positives and negatives, and return them in a 2x2 matrix [[TP, FP], [FN, TN]]."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_confusion_matrix(data, y_hat):\n    confusion_matrix = [[0, 0],[0, 0]]\n    for index in range(len(data)):\n        if y_hat[index] == 1: # predicted value positive\n            if data[index][-1] == 1: # true positive\n                confusion_matrix[0][0] += 1\n            else: # false positive\n                confusion_matrix[0][1] += 1\n        else: # predicted value negative\n            if data[index][-1] == 1: # false negative\n                confusion_matrix[1][0] += 1\n            else: # true negative\n                confusion_matrix[1][1] += 1\n    return confusion_matrix",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 134
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Calculate and return the accuracy using confusion matrix values $\\frac{TP+TN}{n}$. This represents the proportion of total values that were correctly predicted."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_accuracy(conf_matrix):\n    return float(conf_matrix[0][0]+conf_matrix[1][1])/(conf_matrix[0][0]+conf_matrix[0][1]+conf_matrix[1][0]+conf_matrix[1][1])",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Calculate and return the error using confusion matrix values $\\frac{FP+FN}{n}=1-\\frac{TP+TN}{n}$. This represents the proportion of total values that were incorrectly predicted."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_error(conf_matrix):\n    return 1-get_accuracy(conf_matrix)",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 136
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Calculate and return the precision using confusion matrix values $\\frac{TP}{TP+FP}$. This represents the proportion of the predicted positives that were correctly predicted."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_precision(conf_matrix):\n    return float(conf_matrix[0][0])/(conf_matrix[0][0]+conf_matrix[0][1])",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 137
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Calculate and return the recall using confusion matrix values $\\frac{TP}{TP+FN}$. This represents the proportion of the actual positives that were correctly predicted."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_recall(conf_matrix):\n    return float(conf_matrix[0][0])/(conf_matrix[0][0]+conf_matrix[1][0])",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 138
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Train the given data to produce a $\\theta$ vector and calculate the resulting accuracy/error, precision, and recall. Return a list containing the resulting data metrics."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_metrics(test_data, train_data, alpha, epsilon, debug=False):\n    theta = logistic_regression(train_data, alpha, epsilon) # logistic regression theta vector\n    y_hat = get_y_hat(theta, test_data) # estimated y values from theta vector\n    confusion_matrix = get_confusion_matrix(test_data, y_hat)\n    accuracy = get_accuracy(confusion_matrix)\n    precision = get_precision(confusion_matrix)\n    recall = get_recall(confusion_matrix)\n    if debug == True:\n        print \"theta: \" + str(theta)\n        print \"confusion matrix: \" + str(confusion_matrix)\n        print \"accuracy: \" + str(accuracy)\n        print \"error: \" + str(1-accuracy)\n        print \"precision: \" + str(precision)\n        print \"recall: \" + str(recall)\n    return [theta, accuracy, precision, recall]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Calculate the mean value for a metric stored at a particular index."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_avg_metric(metrics, index):\n    if isinstance(metrics[0][index], list): # if a metric is a list, average each item in the list\n        return [sum(metrics[i][index][j] for i in range(len(metrics)))/len(metrics) for j in range(len(metrics[0][index]))]\n    return sum(metrics[i][index] for i in range(len(metrics)))/len(metrics)\n\ntest_metrics = [[1.0, 2.0, [3.0, 4.0], 5.0], [6.0, 7.0, [8.0, 9.0], 10.0]]\nprint get_avg_metric(test_metrics, 2) #test",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "[5.5, 6.5]\n"
      }
     ],
     "prompt_number": 140
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Train the given data incrementally and calculate the error for both the test data and the training data. At each step, increase the amount of training data used, decrease the learning factor, $\\alpha$, and decrease the error change tolerance, $\\epsilon$. Return the resulting learning curves as lists."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_learning_curves(test_data, train_data, training_rate, alpha, epsilon):\n    number_trained = [] # number of data instances used at each training step\n    test_error = [] # test data learning curve\n    train_error = [] # training data learning curve\n    n_train = int(training_rate*len(train_data))\n    n_alpha = alpha\n    n_epsilon = epsilon\n    while n_train < int((1+training_rate)*len(train_data)):\n        theta = logistic_regression(train_data[:n_train], n_alpha, n_epsilon) # perform logistic regression on training data\n        train_y_hat = get_y_hat(theta, train_data) # estimated y values for training data\n        train_error.append(get_error(get_confusion_matrix(train_data, train_y_hat))) # error for the training data\n        test_y_hat = get_y_hat(theta, test_data) # estimated y values for test data\n        test_error.append(get_error(get_confusion_matrix(test_data, test_y_hat)))  # error for the test data\n        number_trained.append(n_train) # size of data trained at this step\n        n_train += int(training_rate*len(train_data))\n        n_alpha = 0.9*n_alpha\n        n_epsilon = epsilon/10\n    return [number_trained, test_error, train_error]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 141
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Plot the average error for the test and training data against the size of the training data set used. When the size of the training data is 0, assume an average error of 0.5 for the test data."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def plot_learning_curves(learning_curves):\n    number_trained = learning_curves[0][0] # size of data set trained\n    test_error = get_avg_metric(learning_curves, 1) # average error in predicted values for test set\n    train_error = get_avg_metric(learning_curves, 2) # average error in predicted values for training set    \n    plt.title('Averaged Learning Curves')\n    plt.plot(([0]+number_trained), ([0.5]+test_error), label=\"test\")\n    plt.plot(([0]+number_trained), ([0]+train_error), label=\"train\")\n    plt.legend(loc='upper right')\n    plt.ylabel('Error')\n    plt.xlabel('Size of Training Data')\n    plt.show() # display plots",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 142
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Perform n-fold (5-fold by default) cross-validation on a set of data, which allows the performance of a model to be tested on a fixed set of data. Shuffle and partition the data in $n$ equal folds. Iterate through each of the $n$ folds, designating the selected partition as the test data and the remaining partitions as the training data for logistic regression. Increment the sample size used on the training data to produce learning curves."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def n_fold(data, n=5, training_rate=0.05, alpha=0.2, epsilon=10**-10, debug=True):\n    if isinstance(data, str): # if a filename is given, read and store the data into a matrix\n        data = read_csv(data)\n    shuffle(data) # shuffle the data in case it is ordered\n    split_data = get_split_data(data, n) # partition the data into n folds\n    metrics = [] # store of data metrics for each test fold\n    learning_curves = [] # store of learning curves for each test fold\n    for i in range(n):\n        test_data = split_data[i] # select a test fold\n        train_data = [x for fold in split_data[:i]+split_data[i+1:] for x in fold]\n        print \"Fold: \" + str(i) # fold number\n        metrics.append(get_metrics(test_data, train_data, alpha, epsilon, debug)) # performance metrics\n        learning_curves.append(get_learning_curves(test_data, train_data, training_rate, 4*alpha, 10**7*epsilon)) # learning curves\n    overall_metrics = get_metrics(data, data, alpha, epsilon)\n    print \"5-fold Average Theta: \" + str(get_avg_metric(metrics, 0))\n    print \"5-fold Average Accuracy: \" + str(get_avg_metric(metrics, 1))\n    print \"5-fold Average Error: \" + str(1-get_avg_metric(metrics, 1))\n    print \"5-fold Average Precision: \" + str(get_avg_metric(metrics, 2))\n    print \"5-fold Average Recall: \" + str(get_avg_metric(metrics, 3))\n    print \"Overall (1-fold) Theta: \" + str(overall_metrics[0])\n    print \"Overall (1-fold) Accuracy: \" + str(overall_metrics[1])\n    print \"Overall (1-fold) Error: \" + str(1-overall_metrics[1])\n    print \"Overall (1-fold) Precision: \" + str(overall_metrics[2])\n    print \"Overall (1-fold) Recall: \" + str(overall_metrics[3])\n    plot_learning_curves(learning_curves)",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#test case\nn_fold('logistic_regression.csv')",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Fold: 0\ntheta: [0.21885020024500929, 1.4383577648745107, 2.105781288976062]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nconfusion matrix: [[12, 0], [3, 5]]\naccuracy: 0.85\nerror: 0.15\nprecision: 1.0\nrecall: 0.8\nFold: 1"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\ntheta: [0.5998181316699822, 1.4990533332625025, 2.453977923914193]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nconfusion matrix: [[7, 3], [2, 8]]\naccuracy: 0.75\nerror: 0.25\nprecision: 0.7\nrecall: 0.777777777778\nFold: 2"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\ntheta: [0.5633789061173577, 1.3985726814255444, 2.6176282071346835]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nconfusion matrix: [[12, 3], [1, 4]]\naccuracy: 0.8\nerror: 0.2\nprecision: 0.8\nrecall: 0.923076923077\nFold: 3"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\ntheta: [0.44140114729963487, 1.458534857560725, 2.2671006757691803]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nconfusion matrix: [[8, 1], [2, 9]]\naccuracy: 0.85\nerror: 0.15\nprecision: 0.888888888889\nrecall: 0.8\nFold: 4"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\ntheta: [0.7509946403358316, 1.4207974529759524, 2.341847226114265]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nconfusion matrix: [[3, 5], [1, 11]]\naccuracy: 0.7\nerror: 0.3\nprecision: 0.375\nrecall: 0.75\n5-fold Average Theta: [0.5148886051335632, 1.4430632180198468, 2.357267064381676]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\n5-fold Average Accuracy: 0.79\n5-fold Average Error: 0.21\n5-fold Average Precision: 0.752777777778\n5-fold Average Recall: 0.810170940171\nOverall (1-fold) Theta: [0.5046251914595904, 1.4335963489122805, 2.3548384488084615]\nOverall (1-fold) Accuracy: 0.81\nOverall (1-fold) Error: 0.19\nOverall (1-fold) Precision: 0.807692307692\nOverall (1-fold) Recall: 0.823529411765\n"
      }
     ],
     "prompt_number": 144
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "**Discussion:**\n\n1. One issue I experienced was the fact that the data set only contained a sample size of 100, which meant that the folds had a noticeable degree of variation, and shuffling the data significantly affected the outcome for each fold. The logistic regression had an overall accuracy of about 80%, which meant that of the values predicted by the model, approximately 4/5 were correct. The precision and recall had similar values, which approximately the same proportion of positive and negative values were correctly predicted, both out of all the predictions made and out of all the actual values. Even through the model performance varied somewhat across the folds, it was consistent enough that I think the model would be likely to exhibit a similar level of performance on new data.\n2. I also noticed that the learning curves varied quite significantly depending upon the shuffling of the data. However, once the values were averaged over the folds, it was clear that the test and training curves always converged rather rapidly, meaning that only a small sample of data is required to achieve an optimal fit to the model as represented by the $\\theta$ vector values. Adding more training data is not likely to improve performance, and in order to further reduce error, we would probably have to create a more accurate model. However, it could also be a possibility that the data used is a poor representation of the problem being solved.\n3. In order to quickly get an accurate result from the logistic regression, I used a modest learning factor and a small error change tolerance ($\\alpha=0.2$ and $\\epsilon=10^{-10}$). However, for very small data sets, it is infeasible to achieve that level of accuracy (it will lead to significant figure boundary issues with $\\hat{y}$ due to the large error). To resolve this issue, I gradually decreased the learning factor ($\\alpha$) and error change tolerance ($\\epsilon$) with each training data size increment."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 144
    }
   ],
   "metadata": {}
  }
 ]
}