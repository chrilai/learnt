{
 "metadata": {
  "name": "clai15_PR12"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Module 12 - Programming Assignment\n=\nThis is the notebook for the Module 10 Programming Assignment.\n\nHere are a few tips for using the iPython HTML notebook:\n\n1.  You can use tab . Try le<&lt;tab> and see the available functions.\n2.  You can change the type of cell by picking \"Code\" or \"Markdown\" from the menu at the left.\n3.  If you keep typing in a Markdown text area, you will eventually get scroll bars. To prevent this, hit return when you come to\nthe end of the window. Only a double return creates a new paragraph.\n4.  You can find out more about Markdown text here: http://daringfireball.net/projects/markdown/ (Copy this link and put it \nin another tab for reference--Don't click it or you'll leave your notebook!).\n5.  Every so often, restart the kernel, clear all output and run all code cells so you can be certain that you didn't\ndefine something out of order.\n\n**You should rename this notebook to be &lt;your JHED id>_PR12\\.ipynb** Do it right now.\n\n**Make certain the entire notebook executes before you submit it.** (See Hint #5 above)\n\nChange the following variables:"
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": "name = \"Christine Lai\"\njhed_id = \"clai15\"\nif name == \"Student Name\" or jhed_id == \"sname1\":\n    raise Exception( \"Change the name and/or JHED ID...preferrably to yours.\")",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Add whatever additional imports you require here. Stick with the standard libraries and those required by the class. The import\ngives you access to these functions: http://ipython.org/ipython-doc/stable/api/generated/IPython.core.display.html (Copy this link)\nWhich, among other things, will permit you to display HTML as the result of evaluated code (see HTML() or display_html())."
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": "from IPython.core.display import *\nfrom StringIO import StringIO\nfrom copy import deepcopy\nfrom clai15_PR11 import read_definitions, read_csv, shuffle_data, get_partitioned_data, get_confusion_matrix, \\\n    get_accuracy, get_precision, get_recall",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "In this assignment you will be using the mushroom data from the Decision Tree module:\n\nhttp://archive.ics.uci.edu/ml/datasets/Mushroom\n\nThe assignment is to program a Naive Bayes Classifier for this data. You'll first need to calculate all of the necessary probabilities (don't forget to use +1 smoothing). You'll then need to have a classify() function that takes an unlabeled instance and returns the normalized probability distribution over the possible classes.\n\nWhich classifier do you prefer for this problem...the decision tree or the naive bayes classifier? Why?"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Initialize a dict to store the counts/probabilities for each occurrence of an attribute value within a label. These will be used to calculate estimated relative probabilities to classify a data instance."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def init_counts(definitions, attributes):\n    counts = {}\n    attribute_sets = []\n    for attribute in attributes: # for each attribute\n        attribute_count = {}\n        for value in definitions[definitions[attribute]]: # initialize attribute value counts for each label\n            attribute_count[value] = 0\n        attribute_sets.append(attribute_count)\n    for label in definitions[definitions[-1]]: # create attribute value counts for each label\n        counts[label] = deepcopy(attribute_sets)\n    return counts",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Initialize a dict to store the counts/probabilities of occurrence of any attribute value within a label. These will be used to calculate estimated relative probabilities to classify a data instance."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def init_label_counts(definitions):\n    label_counts = {}\n    for label in definitions[definitions[-1]]: # create attribute value counts for each label\n        label_counts[label] = 0\n    return label_counts",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "From the given training data, count each occurrence of an attribute value within a label. This will be used to calculate their probabilities."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def set_counts(counts, data):\n    for i in range(len(data[0])): # for each data instance\n        label = data[1][i] # get the label\n        for attribute, value in enumerate(data[0][i]): # count each attribute value\n            counts[label][attribute][value] += 1",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Count each occurrence of any attribute value within a label."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def set_label_counts(label_counts, counts):\n    for label, label_values in counts.items(): # set the total for each label\n        label_counts[label] = sum(n for value, n in label_values[0].items())",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Using the data counts, calculate the probability of occurrence of each class label, $p(c)=\\frac{|c|}{n}$, and of each attribute value within a class label, $p(f_i|c)=\\frac{|f_i,c|+1}{|c|+1}$. Use +1 smoothing to prevent errors from zero value counts."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def set_probabilities(counts, label_counts, probabilities, label_probabilities):\n    for label, label_values in counts.items(): # for each label\n        label_probabilities[label] = float(label_counts[label])/sum(label_counts[i] for i in label_counts)\n        for attribute, attribute_counts in enumerate(label_values): # for each attribute\n            for value, num in attribute_counts.items(): # set probability for each attribute value\n                probabilities[label][attribute][value] = float(num+1)/(label_counts[label]+1)",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Given a data instance, calculate a relative probability value for a label occurring using Naive Bayes classification, $p(c)\\prod\\limits_{j,i}p(a_j=f_i|c)$. This applies the probability chain rule over each attribute, with the assumption of independence."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def probability_of(instance, label, probabilities, label_probabilities):\n    probability = label_probabilities[label] # probability of label\n    for attribute, value in enumerate(instance): # multiply by probability of each attribute value\n        probability *= probabilities[label][attribute][value]\n    return probability",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Normalize a probability distribution such that the sum of the probabilities is 1."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def normalize(distribution):\n    sum_p = sum(distribution[label] for label in distribution)\n    for label in distribution:\n        distribution[label] /= sum_p",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Given a probability distribution, return the label that has the highest probability."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def argmax(distribution):\n    max_p = 0\n    argmax = None\n    for label, value in distribution.items():\n        if value > max_p:\n            max_p = value\n            argmax = label\n    return argmax",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Given a data instance, calculate the relative probabilities of each label, $p(c)\\prod\\limits_{j,i}p(a_j=f_i|c)$. Return a tuple containing the Naive Bayes classification label, $\\newcommand{\\argmax}{\\arg\\!\\max}\\argmax p(c)\\prod\\limits_{j,i}p(a_j=f_i|c)$, along with a dict containing the normalized probability distribution over the labels. Note that the unnormalized distribution is sufficient to classify the instance, as the normalization simply scales the probabilities such that their sum is equal to 1."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_results(probabilities, label_probabilities, instance):\n    results = {}\n    for label in probabilities: # calculate the relative probability for each label\n        results[label] = probability_of(instance, label, probabilities, label_probabilities)\n    best_label = argmax(results)\n    normalize(results)\n    return (best_label, results)",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Create an object to represent a Naive Bayes classifier, which uses the probabilities of each attribute value in a class label to determine the overall relative probabilities of each label. This allows data with large numbers of attributes to be efficiently modeled and classified. The initialization function creates data structures to store the counts and probabilities for each label and attribute value. The train function populates the data structures using the given training data. The classify function returns the label based on the argmax of the relative probabilities and a normalized probability distribution for a given test instance."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "class BayesClassifier(object):\n    \n    def __init__(self, definitions='agaricus-lepiota.names.data'):\n        if isinstance(definitions, str): # if a filename is given, read and store the definitions into a dict\n            definitions = read_definitions(definitions)\n        self.definitions = definitions\n        self.num_attributes = len(definitions)/2 - 1\n        self.counts = init_counts(self.definitions, range(self.num_attributes))\n        self.probabilities = deepcopy(self.counts)\n        self.label_counts = init_label_counts(self.definitions)\n        self.label_probabilities = deepcopy(self.label_counts)\n    \n    def train(self, data='agaricus-lepiota.data'):\n        if isinstance(data, str): # if a filename is given, read and store the data into a list of lists\n            data = read_csv(data)\n        set_counts(self.counts, data) # get the counts for each attribute value within a class label\n        set_label_counts(self.label_counts, self.counts) # get the counts for each class label\n        set_probabilities(self.counts, self.label_counts, self.probabilities, self.label_probabilities)\n        \n    def classify(self, instance):\n        return get_results(self.probabilities, self.label_probabilities, instance)",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# test classification of known and modified instances\nbayes_classifier = BayesClassifier()\nbayes_classifier.train()\n#print bayes_classifier.counts\n#print bayes_classifier.label_counts\n#print bayes_classifier.probabilities\n#print bayes_classifier.label_probabilities\nprint \"known poisonous: \" + str(bayes_classifier.classify(['x','s','n','t','p','f','c','n','k','e','e','s','s','w','w','p','w','o','p','k','s','u']))\nprint \"test edible: \" + str(bayes_classifier.classify(['x','s','n','t','a','f','c','n','k','e','e','s','s','w','w','p','w','o','p','k','s','u']))\nprint \"known edible: \" + str(bayes_classifier.classify(['x','s','g','f','n','f','w','b','k','t','e','s','s','w','w','p','w','o','e','n','a','g']))\nprint \"test poisonous: \" + str(bayes_classifier.classify(['x','s','g','f','n','f','w','b','k','t','e','s','s','w','w','p','w','o','e','r','a','g']))",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "known poisonous: ('e', {'p': 0.29240030820945595, 'e': 0.7075996917905442})\ntest edible: ('e', {'p': 4.0096914422602e-06, 'e': 0.9999959903085577})\nknown edible: ('e', {'p': 1.1277631476618882e-08, 'e': 0.9999999887223685})\ntest poisonous: ('e', {'p': 6.384853019394097e-06, 'e': 0.9999936151469806})\n"
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "(Modified from PR9) Train the given data to produce a Naive Bayes classifier and calculate the resulting accuracy/error, precision, and recall. Return a list containing the resulting data metrics."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_metrics(test_data, train_data, debug=False, positive='p'):\n    bayes_classifier = BayesClassifier()\n    bayes_classifier.train()\n    y = test_data[1]\n    y_hat = [bayes_classifier.classify(x)[0] for x in test_data[0]]\n    confusion_matrix = get_confusion_matrix(y, y_hat, positive)\n    accuracy = get_accuracy(confusion_matrix)\n    precision = get_precision(confusion_matrix)\n    recall = get_recall(confusion_matrix)\n    print \"confusion matrix: \" + str(confusion_matrix)\n    if debug == True:\n        print \"accuracy: \" + str(accuracy)\n        print \"error: \" + str(1-accuracy)\n        print \"precision: \" + str(precision)\n        print \"recall: \" + str(recall)\n    return [accuracy, precision, recall]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "(Copied from PR11) Perform n-fold (10-fold by default) cross-validation on a set of data, which allows the performance of a model to be tested on a fixed set of data. Shuffle and partition the data in $n$ equal folds. Iterate through each of the $n$ folds, designating the selected partition as the test data and the remaining partitions as the training data for logistic regression."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def n_fold(n=10, data='agaricus-lepiota.data', debug=False):\n    if isinstance(data, str): # if a filename is given, read and store the data into a matrix\n        data = read_csv(data)\n    metrics = [] # store of data metrics for each test fold\n    if n == 1:\n        metrics.append(get_metrics(data, data)) # performance metrics\n    else:\n        data = shuffle_data(data) # shuffle the data in case it is ordered\n        partitioned_data = get_partitioned_data(data, n) # partition the data into n folds\n        for i in range(n):\n            print \"Fold: \" + str(i)\n            test_data = partitioned_data[i] # select a test fold\n            train_data_x = [x for fold in partitioned_data[:i]+partitioned_data[i+1:] for x in fold[0]] # select a training fold\n            train_data_y = [y for fold in partitioned_data[:i]+partitioned_data[i+1:] for y in fold[1]] # select a training fold\n            metrics.append(get_metrics(test_data, [train_data_x, train_data_y])) # performance metrics\n    print str(n) + \"-fold Average Accuracy: \" + str(sum(metrics[i][0] for i in range(n))/n)\n    print str(n) + \"-fold Average Error: \" + str(sum(1-metrics[i][0] for i in range(n))/n)\n    print str(n) + \"-fold Average Precision: \" + str(sum(metrics[i][1] for i in range(n))/n)\n    print str(n) + \"-fold Average Recall: \" + str(sum(metrics[i][2] for i in range(n))/n)",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# run 1-fold and 10-fold tests to measure model proficiency\nn_fold(10)\n#n_fold(5)\n#n_fold(2)\nn_fold(1)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Fold: 0\nconfusion matrix: [[357, 3], [29, 424]]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nFold: 1\nconfusion matrix: [[338, 2], [32, 441]]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nFold: 2\nconfusion matrix: [[361, 1], [34, 417]]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nFold: 3\nconfusion matrix: [[368, 3], [33, 409]]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nFold: 4\nconfusion matrix: [[358, 0], [40, 414]]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nFold: 5\nconfusion matrix: [[332, 1], [39, 440]]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nFold: 6\nconfusion matrix: [[382, 2], [32, 396]]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nFold: 7\nconfusion matrix: [[356, 4], [29, 423]]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nFold: 8\nconfusion matrix: [[356, 0], [30, 426]]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nFold: 9\nconfusion matrix: [[376, 4], [34, 398]]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\n10-fold Average Accuracy: 0.956670998976\n10-fold Average Error: 0.043329001024\n10-fold Average Precision: 0.994508686618\n10-fold Average Recall: 0.915112356193\nconfusion matrix: [[3584, 20], [332, 4188]]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\n1-fold Average Accuracy: 0.95667159035\n1-fold Average Error: 0.0433284096504\n1-fold Average Precision: 0.994450610433\n1-fold Average Recall: 0.915219611849\n"
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "***Comments/Observations:***\n\nAs I observed in PR11, the dataset tested in this assignment works rather well with a decision tree model, as it is internally consistent and complete enough to achieve practically flawless 10-fold accuracy. The decision tree model is also rather efficient for this dataset, as not all of the attributes are required to classify an instance using the tree. Since this particular problem involves the identification of poisonous (versus edible) mushrooms, it would be more desirable to have more false positives than false negatives. In terms of metrics, we would want to have a high accuracy, but we would also want to focus on to maximizing recall (the proportion of actual positives that were correctly predicted), and possibly sacrifice some precision (the proportion of predicted positives that were correctly predicted) if needed.\n\nThe Naive Bayes classifier did not perform as well as the decision tree model did on all of these measures of performance, and particularly not with regards to recall. This is due to the fact that instead of definitively separating the data by specific attribute values, it calculates the relative probabilities the attribute values in each label. This means that the probabilities for each attribute are assumed to be independent, and that both the labels and the attribute values within a label that have higher rates of occurrence are weighted more strongly than those with lower rates of occurrence. To improve the probability estimates, we would need to determine which attributes are dependent upon one another. In general, I think that the Bayes classification technique would perform better on data that contains many attributes but is not as internally consistent (there are many combinations of attribute values that can result in multiple labels, preventing it from being effectively partitioned), so that an efficient estimate needs to be made over all attributes."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    }
   ],
   "metadata": {}
  }
 ]
}