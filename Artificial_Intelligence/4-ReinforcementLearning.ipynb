{
 "metadata": {
  "name": "clai15_PR4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#Module 4 - Programming Assignment\n\nThis is the notebook for the Module 4 Programming Assignment.\n\nHere are a few tips for using the iPython HTML notebook:\n\n1.  You can use tab . Try le<&lt;tab> and see the available functions.\n2.  You can change the type of cell by picking \"Code\" or \"Markdown\" from the menu at the left.\n3.  If you keep typing in a Markdown text area, you will eventually get scroll bars. To prevent this, hit return when you come to\nthe end of the window. Only a double return creates a new paragraph.\n4.  You can find out more about Markdown text here: http://daringfireball.net/projects/markdown/ (Copy this link and put it \nin another tab for reference--Don't click it or you'll leave your notebook!).\n5.  Every so often, restart the kernel, clear all output and run all code cells so you can be certain that you didn't\ndefine something out of order.\n\n**You should rename this notebook to be &lt;your JHED id>\\_PR1.ipynb** Do it right now.\n\n**Make certain the entire notebook executes before you submit it.** (See Hint #5 above)\n\nChange the following variables:"
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": "name = \"Christine Lai\"\njhed_id = \"clai15\"\nif name == \"Student Name\" or jhed_id == \"sname1\":\n    raise Exception( \"Change the name and/or JHED ID...preferrably to yours.\")",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Add whatever additional imports you require here. Stick with the standard libraries and those required by the class. The import\ngives you access to these functions: http://ipython.org/ipython-doc/stable/api/generated/IPython.core.display.html (Copy this link)\nWhich, among other things, will permit you to display HTML as the result of evaluated code (see HTML() or display_html())."
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": "from copy import deepcopy\nfrom IPython.core.display import *\nfrom StringIO import StringIO\nfrom random import choice\nfrom random import random",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "This problem is very similar to the one in Module 1 that we solved with A\\* search. The main difference is that we now have stochastic movement (actions don't always result in the desired successor state) so we can't use A\\* search to solve it. Instead you're going to use Q-Learning (reinforcement learning).\n\nAs before, we're going to simplify the problem by working in a grid world. The symbols that form the grid have a special meaning as they\nspecify the type of the terrain and the cost to enter a grid cell with that type of terrain:\n\n<code>\ntoken   terrain    cost \n.       plains     1\n*       forest     3\n^       hills      5\n~       swamp      7\nx       mountains  impassible\n</code>\n\nWhen you go from a plains node to a forest node it costs 3. When you go\nfrom a forest node to a plains node, it costs 1. You can think of the grid as a big graph. Each grid cell (terrain symbol)\nis a node and there are edges to the north, south, east and west (except at the edges). Unlike A* Search, Reinforcement Learning\nwill find a policy for the entire state space. The goal is (26, 26) with a reward of 100.0.\n\nTo avoid global variables, we have a <code>read_world()</code> function that takes a filename and returns the world as list of lists. The same coordinates reversal applies: (x, y) is world[ y][ x]."
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": "def read_world( filename):\n    with open( filename, 'r') as f:\n        world_data = [x for x in f.readlines()]\n    f.closed\n    world = []\n    for line in world_data:\n        line = line.strip()\n        if line == \"\": continue\n        world.append([x for x in line])\n    return world",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Next we create a dict of movement costs. Note that we've negated them this time because RL requires negative costs and positive rewards:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# do not reference this as a global variable.\ncosts = { '.': -1, '*': -3, '^': -5, '~': -7}\ncosts",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": "{'*': -3, '.': -1, '^': -5, '~': -7}"
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "and a list of offsets for NEIGHBORS. You'll need to work this into your actions, A, parameter."
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": "NEIGHBORS = [(0,-1), (1,0), (0,1), (-1,0)]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The transition function, T, is 0.70 for the desired direction, and 0.10 each for the other possible directions. That is, if I select \"up\" then 70% of the time, I go up but 10% of the time I go left, 10% of the time I go right and 10% of the time I go down. If you're at the edge of the map, you simply bounce back to the current state.\n\nYou need to implement <code>q_learning()</code> with the following parameters:\n\n+ world: a list of lists of terrain (this is S from S, A, T, gamma, R)\n+ costs: a hash of costs by terrain (this is part of R)\n+ goal: A Tuple of (x, y) stating the goal state.\n+ reward: The reward for achieving the goal state.\n+ actions: a List of possible actions, A.\n+ gamma: the discount rate\n+ alpha: the learning rate\n\nyou will return a policy: {(x1, y1): action1, (x2, y2): action2, ...} Remember...a policy is what to do in any/every state. Notice how this is different that A\\* search which only returns one path from start to goal. For a 2d navigation problem, you can also print out a map of what action to do in every state.\n\nUse a goal that is the lower right hand corner of the world.\n\nThere are a lot of details that I have left up to you. Watch and re-watch the lecture on Q-Learning. Ask questions. You need to implement a way to pick initial states for each iteration and you need a way to balance exploration and exploitation while learning. You may have to experiment with different gamma and alpha values.\n\nRemember that you should follow the general style guidelines for this course: well-named, short, focused functions with limited indentation using Markdown documentation that explains their implementation and the AI concepts behind them.\n\nThis assignment sometimes wrecks havoc with IPython notebook. Create a small test world (5x6) to work with. Pick an asymmetric world to avoid edge cases."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Helper function to get the x coordinate of an (x, y) node location."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_x( node):\n    return node[0]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Helper function to get the y coordinate of an (x, y) node location."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_y( node):\n    return node[1]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Return the Manhattan distance of a node from the origin. This may be used to sort the nodes in a world."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_distance( node):\n    return node[0] + node[1]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Helper function to get the value stored in a matrix corresponding to an (x, y) node location. For a grid world represented as a list of lists, this returns the token representing the type of terrain at an (x, y) node, where x-axis goes across from left to right and the y-axis goes down. This is used to check whether the terrain is passable and to retrieve the cost of entering the terrain."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_value( matrix, node):\n    return matrix[node[1]][node[0]]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Helper function to store a value in a matrix corresponding to an (x, y) node location."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def set_value( matrix, node, value):\n    matrix[node[1]][node[0]] = value",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Helper function to retrieve a copy of a list, excluding an item."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_other_list(list, item):\n    other_list = deepcopy(list)\n    other_list.remove(item)\n    return other_list",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Check whether a coordinate is within the bounds of the grid and has passable terrain (not mountains)."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def isPassable( world, node):\n    x_limit = len(world[0])\n    y_limit = len(world)\n    if node[0] >= 0 and node[0] < x_limit and node[1] >= 0 and node[1] < y_limit:\n        if get_value(world, node) != 'x':\n            return True\n    return False",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Define a variable mapping the four possible directions for movement on a grid to coordinate values.\n"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "DIRECTIONS = {'up':(0,-1), 'right':(1,0), 'down':(0,1), 'left':(-1,0)}",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Given a node (state s), return a neighboring node (state s') in the direction indicated by the given action (a). If the node in that direction countains mountains or does not exist in the world, then bounce back to the original node. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_next_node( world, node, action):\n    next_node = (node[0] + DIRECTIONS[action][0], node[1] + DIRECTIONS[action][1])\n    if not isPassable(world, next_node): # if the node being moved to is not passable, bounce back to the original node\n        return node\n    return next_node",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Return the reward R(s, a, s') = R(s, a) of moving to a on the grid with an intended action a. We define this value to be independent of the state resulting from the actual action taken. *Function is not used in implementation - see alternate function below.*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_r( costs, world, node, action):\n    next_node = get_next_node(world, node, action)\n    if node == next_node: # bounced back to same node\n        return 0\n    return costs[get_value(world, next_node)]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Alternate function: Return the reward R(s, a, s') = R(s') of moving to a node on the grid. We define this value to be independent of the original state s from which the action a was taken."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_r_sarsa( costs, world, node, next_node):\n    if node == next_node: # bounced back to same node\n        return 0\n    return costs[get_value(world, next_node)]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Return the greatest approximated reward for a given state s, as determined by the current maximum value of of Q(s,a)."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_q_max( q_matrix, node):\n    q_max = None\n    for action in q_matrix:\n        value = get_value(q_matrix[action], node)\n        if q_max == None or q_max < value:\n            q_max = value\n    return q_max",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Return the action a that will result in the greatest approximated reward for a given state s, as determined by the current maximum value of of Q(s,a). If there are multiple optimal actions, select a random action. Return null if the current state is the goal state or if no action can be taken."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def argmax( world, goal, q_matrix, node):\n    max_a = None\n    q_max = None\n    if node == goal: # if the current state is the goal, no action should be taken\n        return None\n    for action in q_matrix:\n        next_node = get_next_node(world, node, action)\n        if node == next_node: # if the action leads to an impassable node, do not take it\n            continue\n        q_value = get_value(q_matrix[action], node)\n        if q_max == None or q_max < q_value: # select action with max reward\n            max_a = action\n            q_max = q_value\n        elif q_max == q_value: # if multiple optimal actions, select action that will lead closer to goal\n            if get_distance(next_node) > get_distance(node):\n                max_a = action\n            q_max = q_value\n    return max_a",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Return a random action a to explore nodes. This path is taken under the $\\epsilon$-greedy algorithm, which gives a random chance that a random node will be explored instead of taking the greedy path given by argmax."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def epsilon_action( world, actions, node):\n    action_pool = deepcopy(actions)\n    for action in actions:\n        next_node = get_next_node(world, node, action)\n        if node == next_node: # if the action leads to an impassable node, do not take it\n            action_pool.remove(action)\n    if len(action_pool) == 0:\n        return None\n    else:\n        return choice(action_pool) # select random avaiable action",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Return the new Q-value approximation Q(s,a) for a taken action. The values chosen to update the Q matrix are those resulting from the actual action taken (SARSA - state, action, reward, state, action), as opposed to those that would result from the intended action."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_q( world, costs, gamma, alpha, q_matrix, node, action, next_node):\n    q_value = get_value(q_matrix[action], node) # current value of Q(s,a)\n    q_value_next = get_q_max(q_matrix, next_node) # current value of Q(s',a)\n    #r_value = get_r(costs, world, node, action) # reward R(s')\n    r_value = get_r_sarsa(costs, world, node, next_node) # reward R(s')\n    return (1-alpha)*q_value + alpha*(r_value + gamma*q_value_next)",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Simulate the transition function, T(s, a, s'), which is unknown to the agent. From a node (state s), moving one space (action a) should result in the neighboring node in that direction (state s'). However, each of the other actions is taken some (10%) of the time and the intended action is only taken the remainder (70%) of the time."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def transition( world, costs, actions, gamma, alpha, q_matrix, node, action):\n    other_actions = get_other_list(actions, action)\n    random_num = random() # random value: [0,1)\n    if random_num < 0.10: # 10% chance of taking unintended direction\n        next_node = get_next_node(world, node, other_actions[0])\n    elif random_num < 0.20: # 10% chance of taking unintended direction\n        next_node = get_next_node(world, node, other_actions[1])\n    elif random_num < 0.30: # 10% chance of taking unintended direction\n        next_node = get_next_node(world, node, other_actions[2])\n    else: # 70% chance of taking intended direction\n        next_node = get_next_node(world, node, action)\n    q_value_new = get_q(world, costs, gamma, alpha, q_matrix, node, action, next_node) # calculate adjusted value for Q(s,a)\n    set_value(q_matrix[action], node, q_value_new) # update value for Q(s,a)\n    return next_node",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Return a list of start nodes to use for Q-learning episodes. Each iteration of the Q-learning process starts at a node and explores nodes until a goal or terminal state is reached. We can try iterating through all nodes by distance from the goal, and iterating through all rows and columns."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_episodes( world):\n    episodes = []\n    all_nodes = []\n    for x in range(len(world[0])):\n        for y in range(len(world)):\n            all_nodes.append((x, y))\n            all_nodes.append((x, y))\n    xy_nodes = sorted(all_nodes, key=get_distance, reverse=True)\n    episodes.extend(xy_nodes)\n    x_nodes = sorted(all_nodes, key=get_x, reverse=True)\n    episodes.extend(x_nodes)\n    y_nodes = sorted(all_nodes, key=get_y, reverse=True)\n    episodes.extend(y_nodes)\n    episodes.extend(episodes)\n    episodes.extend(episodes)\n    #episodes.extend(episodes)\n    return episodes",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Return a dict representing the policy $\\pi$(s), which gives the optimal action that should be taken at each state based upon the argmax for each (x, y) node location in the matrix of Q-values."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_policy( world, goal, actions, q_matrix):\n    pi = {}\n    for x in range(len(world[0])):\n        for y in range(len(world)):\n            node = (x, y)\n            action = argmax(world, goal, q_matrix, node)\n            if action == None:\n                pi[node] = 'none'\n            else:\n                pi[node] = action\n    return pi",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Given a policy for a grid world, print a map of actions to take in every state."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def print_policy( world, policy):\n    for y in range(len(world)):\n        for x in range(len(world[0])):\n            print policy[(x, y)],\n        print\n    print",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "This is the main function that calls the Q-learning algorithm, which can be used to discover the best actions to take at every state of a search problem when the transition model is unknown. First, initialize the Q-value matrix to 0 and set the goal reward value. Next, retrieve a list of nodes with which to start exploration episodes. Then explore the nodes using an epsilon-greedy algorithm in which the greedy actions are chosen based on argmax, with a chance of exploring a random node. The q values are stored in a set of matrices representing each possible action, and the highest value for the state at each node determines the approximated optimum choice for that state.\n\nThe value of gamma represents the discount factor, which determines how highly future feedback is valued compared to current knowledge. The value of alpha represents the learning rate, which affects how highly new results are valued compared to old results. The Q-learning episodes are run over some number of episodes to improve our knowledge of the grid world and transition model. Finally, the policy $\\pi$(s) is retrieved from the matrix of Q-values."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def q_learning( world, costs, goal, reward, actions, gamma, alpha):\n    epsilon = 0.2 # exploration chance\n    q_matrix = {} # store a matrix of values Q(s, a) for all a\n    for action in actions:\n        matrix = [[0 for x in range(len(world[0]))] for y in range(len(world))] # initialize Q(s,a) = 0\n        set_value(matrix, goal, reward) # set reward value at goal node\n        q_matrix[action] = matrix\n    print \"Initial Q(s, a):\" + str(q_matrix) + \"\\n\" #trace\n    for node in get_episodes( world):\n        while node != goal:\n            if random() < epsilon:\n                action = epsilon_action(world, actions, node)\n            else:\n                action = argmax(world, goal, q_matrix, node)\n            if action == None: # if no possible action can or should be taken\n                break\n            node = transition(world, costs, actions, gamma, alpha, q_matrix, node, action)\n    print \"Final Q(s, a):\" + str(q_matrix) + \"\\n\" #trace\n    return get_policy(world, goal, actions, q_matrix)",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Test statements:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "COSTS = { '.': -1, '*': -3, '^': -5, '~': -7} # the costs for moving to each type of terrain\nREWARD = 100.0 # reward for achieving goal state\nACTIONS = ['up','right','down','left']\nworld = read_world('test_world.txt') # test world\ngoal = (len(world[0])-1,len(world)-1) # last cell in grid world\ngamma = .99\nalpha = 0.05\npolicy = q_learning( world, COSTS, goal, REWARD, ACTIONS, gamma, alpha)\nprint \"Test World 0 Policy: \" + str(policy) + \"\\n\"\nprint_policy(world, policy)\nworld = read_world('test_world_alt.txt') # test world\ngoal = (len(world[0])-1,len(world)-1) # last cell in grid world\ngamma = 0.99\nalpha = 0.05\npolicy = q_learning( world, COSTS, goal, REWARD, ACTIONS, gamma, alpha)\nprint \"Test World 1 Policy: \" + str(policy) + \"\\n\"\nprint_policy(world, policy)\n#world = read_world('world.txt') # test world\n#goal = (len(world)-1,len(world)-1) # last cell in grid world\n#gamma = 0.9\n#alpha = 0.25\n#policy = q_learning( world, COSTS, goal, REWARD, ACTIONS, gamma, alpha)\n#print \"Test World 2 Policy: \" + str(policy) + \"\\n\"\n#print policy(world, policy)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Initial Q(s, a):{'down': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 100.0]], 'right': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 100.0]], 'up': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 100.0]], 'left': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 100.0]]}\n\nFinal Q(s, a):{'down': [[-1.9190250093375827, 13.264140063905439, 1.2071841256530873, -2.479981318727546, 2.4387888798544655, 14.863812276920433, 57.969779411050396], [-1.961563211784114, 2.378143877893688, 37.38082128467055, 42.42145335405308, 8.578203684297094, 58.39794444700404, 65.75349753785969], [-2.7265339248791585, -3.1798957976369, 8.399571345836282, 58.21311605885119, 61.299790715764246, 34.40071500654851, 75.13668771167254], [-2.50003543330589, -1.6317894440683383, 38.99961262708978, 39.782997121405394, 75.25564214692803, 80.0327555303864, 85.34643990980128], [17.7029499020671, 37.030719466098056, 54.209536210972225, 21.101235531867026, 53.47458806819347, 74.40817247551365, 91.8785085576903], [36.811300243892425, 11.955329917640858, 66.98809579390226, 72.62337776397247, 82.2445310607504, 88.34479189030523, 95.56344336897081], [0, 0, 0, 0, 0, 0, 100.0]], 'right': [[4.281488951412706, -1.7166014431835674, -0.4699596502969494, 19.10243189644206, 31.858811872639528, 46.7246522403293, 0], [9.140744504763424, 24.378125472230533, 7.060309104385176, 2.7149043329777194, 41.1898559197061, 21.749668680057635, 0], [11.921493207600596, 28.553345411743496, 50.37022077124029, 23.01304578352498, 10.993248165681493, 68.2846059895244, 0], [-4.29997444190749, 0.015780062636285175, 7.948610145694168, 66.81129521045622, 43.17419113144141, 41.13058783513939, 0], [-2.0656761706287456, 4.195625362511942, 16.737323159218388, 73.82173305303401, 79.97600951065564, 84.59697704968049, 0], [0.3300224552156396, 53.59144722887196, 30.055256751492063, 25.359678956930264, 38.0789524641882, 48.409465259257765, 0], [51.94486091320408, 63.77459296555873, 72.74495617196403, 79.2964574748904, 88.042350176105, 94.30270285770506, 100.0]], 'up': [[0, 0, 0, 0, 0, 0, 0], [-1.2387884455007812, 4.062527835921163, 0.5153730627981861, 0.5367978244950125, 0.0715294640555052, 2.1184857309505447, 28.50508214475454], [-1.9133552095099748, 5.621406627341969, 8.862290943710818, 11.164093258772855, 4.681648989099202, 23.06689034322578, 46.671841232858064], [3.579532063868166, -3.5600199475606615, 1.8923567434955144, 20.489810704052953, 37.030286897670024, 36.12778457197665, 57.96839299290163], [-1.4997694038183216, -3.802007930062872, 4.21428254253598, 21.401700600819765, 48.86649174244074, 58.456839304914226, 80.42684217122977], [-1.1756558750333834, 2.832602209142132, 23.62893590409255, 20.808268179657055, 22.276207994680952, 52.81953071309294, 81.1010622992056], [10.368457435787436, 13.145050273671409, 52.027453830177485, 59.89627708305794, 74.81643460988576, 82.89559882602231, 100.0]], 'left': [[0, -0.7137596350203417, 7.979578974851549, -2.066987520153801, 3.7138507576502455, 6.600563390520444, 21.73320323954962], [0, 1.1195556898478967, 3.7700455253802154, -1.3687594748777125, 2.091900436404744, 3.5850247403765696, 28.286279066685474], [0, 0.8208927565187516, 6.227595661171647, 21.575290546255037, 16.290368573501016, 20.458433148136017, 40.29709034845272], [0, 8.435767369842917, -1.2370736581180566, 20.676032775056242, 30.69862569757361, 31.58754605025858, 62.692566910847304], [0, -2.455623362788831, 7.808689350844277, 14.506489148966796, 45.796630489924915, 59.85628758890239, 81.55257210829015], [0, 5.093988879833424, 16.553099076484752, 22.906580986815005, 19.08886798932339, 47.567320034019275, 83.27567374947007], [0, 18.89399615720263, 46.00285231100744, 62.475282113428634, 71.69149643401217, 84.92748782403716, 100.0]]}\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nTest World 0 Policy: {(1, 3): 'left', (6, 6): 'none', (3, 0): 'right', (5, 4): 'right', (2, 1): 'down', (5, 6): 'right', (2, 6): 'right', (1, 6): 'right', (5, 1): 'down', (2, 5): 'down', (0, 3): 'up', (4, 0): 'right', (1, 2): 'right', (3, 3): 'right', (2, 0): 'left', (4, 4): 'right', (6, 3): 'down', (1, 5): 'right', (3, 6): 'right', (2, 2): 'right', (5, 3): 'down', (4, 1): 'right', (1, 1): 'right', (6, 4): 'down', (3, 2): 'down', (0, 0): 'right', (5, 0): 'right', (4, 5): 'down', (0, 4): 'down', (5, 5): 'down', (1, 4): 'down', (6, 0): 'down', (2, 3): 'down', (4, 2): 'down', (1, 0): 'down', (6, 5): 'down', (3, 5): 'down', (0, 1): 'right', (4, 6): 'right', (5, 2): 'right', (6, 1): 'down', (3, 1): 'down', (0, 2): 'right', (0, 6): 'right', (6, 2): 'down', (4, 3): 'down', (0, 5): 'down', (3, 4): 'right', (2, 4): 'down'}\n\nright down left right right right down\nright right down down right down down\nright right right down down right down\nup left down right down down down\ndown down down right right right down\ndown right down down down down down\nright right right right right right none\n\nInitial Q(s, a):{'down': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 100.0]], 'right': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 100.0]], 'up': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 100.0]], 'left': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 100.0]]}\n\nFinal Q(s, a):{'down': [[-0.6916114501174357, 0.46334041603454956, 0, 60.255844168943234, 55.762573518865935, 0, 0], [-0.4146487972363027, 2.088803193901066, 0, 68.12846523646634, 66.26486326635636, 9.055692135240214, 0], [-1.7971536103903185, 30.24281676792144, -0.05, 73.03183924201814, 76.01242870279414, 80.4232640218013, 49.118265084215636], [30.380501398698577, 46.28892892563064, 38.159744845494366, 50.616093523160906, 82.07622296440017, 86.63575111477155, 90.16625657421632], [5.346471927896172, 0, 0, 0, 80.23695704619607, 86.87353101034633, 95.14093192150943], [0, 0, 0, 0, 0, 0, 100.0]], 'right': [[8.489088844737719, 31.873091048867053, 48.479251772439525, 18.544248098448787, 4.981031442630302, 0, 0], [3.0369642657897633, 0, 22.773888385840483, 30.243633298903394, 0, 0, 0], [13.802309936983542, 0, 28.771293103836193, 38.069632084756435, 26.615716737709757, 0, 0], [-0.6832415588302618, 10.344625406212106, 67.54073059257259, 76.79298130189575, 61.95644367526945, 48.95388805773848, 0], [47.40379172936727, 59.27978625092389, 74.05241684674476, 81.10224391792376, 85.76744153433356, 90.26835011283563, 0], [0, 0, 0, 51.617424440650915, 92.3381841028973, 95.21863083076632, 100.0]], 'up': [[0, 0, 0, 0, 0, 0, 0], [-0.6409003013766056, 17.526673997473996, 6.266675230408465, 26.52498943140913, 16.95136780709731, -0.06057780160282167, 0], [-2.1066466894605678, 0.4751142540093395, 0, 39.3181768977706, 30.137695233327804, 0, 0], [-1.1996779169206009, 1.966272315307831, 0, 35.80580013786852, 56.81380409721359, 46.182032216471114, 0], [3.3794549379920458, 23.107991468673482, 41.38872784414752, 62.22759977193008, 69.80996736821837, 77.83469081932317, 87.74001356929112], [30.289545665818537, 22.046684165750342, 34.429696044618765, 5.211476822375744, 59.883675520825705, 60.12486840925634, 100.0]], 'left': [[0, 1.3793124858490418, 12.747734019232762, 9.732708930755859, 10.561996325179397, 38.61122898673044, 8.237716800484282], [0, -2.437995711086616, -0.0975, 0, 28.81462011150459, 25.091120966682958, 0], [0, 0.7885217806289062, 1.2418411012240125, 0, 35.40915589554285, 29.869471318994524, 6.261462498534327], [0, 2.5851515450335576, 16.192961089368936, 43.22790662402242, 51.43751945168094, 44.11559622500707, 61.44921049798666], [0, 27.66108298982331, 39.11494815898918, 53.343782647118815, 71.67126414311731, 76.95102853218903, 86.5014275675458], [0, 2.3714869879190643, 0, 0, 0, 66.71530001489248, 100.0]]}\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nTest World 1 Policy: {(1, 3): 'down', (3, 0): 'down', (5, 4): 'right', (2, 1): 'right', (6, 2): 'down', (5, 1): 'left', (2, 5): 'up', (0, 3): 'down', (4, 0): 'down', (1, 2): 'down', (3, 3): 'right', (4, 4): 'right', (6, 3): 'down', (1, 5): 'up', (5, 0): 'left', (2, 2): 'right', (5, 3): 'down', (4, 1): 'down', (1, 1): 'up', (6, 4): 'down', (3, 2): 'down', (0, 0): 'right', (4, 5): 'right', (0, 4): 'right', (5, 5): 'right', (1, 4): 'right', (6, 0): 'left', (0, 5): 'up', (4, 2): 'down', (1, 0): 'right', (6, 5): 'none', (3, 5): 'right', (0, 1): 'right', (5, 2): 'down', (6, 1): 'none', (3, 1): 'down', (0, 2): 'right', (2, 0): 'right', (4, 3): 'down', (2, 3): 'right', (3, 4): 'right', (2, 4): 'right'}\n\nright right right down down left left\nright up right down down left none\nright down right down down down down\ndown down right right down down down\nright right right right right right down\nup up up right right right none\n\n"
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The final cell below, \"write something here\", should include your comments, observations, challenges, thoughts about the assignment, what you learned or what you still find confusing. Write 1-2 paragraphs."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "For this assignment, I found my greatest challenge to be figuring out how to tweak the Q-learning algorithm to more quickly have the approximate solution approach the actual statistical values for the transition model. I noticed that one of the biggest issues encountered with an \"intermediate\" solution was that the resulting policies would containin intraverseable paths near the borders of the world. When tweaking gamma and alpha values, I observed that a high gamma value improved the results for the states farthest from the goal state.  Meanwhile, too high of an alpha value prevented an equilibrium state from ever being approached due to the stochastic nature of the transition model, which can produce unexpected action outcomes.\n\nTo try to improve the values faster, I implemented an epsilon-greedy approach to explore new random paths, and created episodes that explored the map not only at increasing distances, but also row by row and column by column. I also calculated the reward values based upon the resultant state for each actual actios taken rather than each intended action. Overall, the most important factor  seemed to be the methods for balancing exploration and exploitation and selecting sufficient episodes to produce a good outcome. On larger maps than those tested here, the number of iterations required becomes a burden, and more selectivity might be required."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# my testing",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    }
   ],
   "metadata": {}
  }
 ]
}